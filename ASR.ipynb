{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "NMFWCJYW_egp"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/taimoorsardar/Automatic-Speech-Recognition/blob/main/ASR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "in this notebook we will try to fine tune one of the state-of-the-art model Whisper A Seq2Seq Transformer model on common voice Urdu dataset.\n",
        "this model is recently open sourced by openAi.\n",
        "Morover we tried different variants separately, means just changed the model in this notebook."
      ],
      "metadata": {
        "id": "E3gv2oNsuERZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing Environment"
      ],
      "metadata": {
        "id": "WrSObheZsJ_E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking gpu\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ],
      "metadata": {
        "id": "EU9JtJYAu_pP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VUOVBBn30jL"
      },
      "outputs": [],
      "source": [
        "# updating and installing required libraries, as suggested in hugging face\n",
        "!pip install --upgrade pip\n",
        "!pip install --upgrade datasets[audio] transformers accelerate evaluate jiwer tensorboard gradio"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as we are using hugginface as our source of picking the dataset so we need to login from there as well\n"
      ],
      "metadata": {
        "id": "sweadFGvEH6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "-uX3aF5M2zqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Load Dataset"
      ],
      "metadata": {
        "id": "6PRSq2tk23ky"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "\n",
        "common_voice = DatasetDict()\n",
        "\n",
        "common_voice[\"train\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"ur\", split=\"train+validation\", use_auth_token=True)\n",
        "common_voice[\"test\"] = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"ur\", split=\"test\", use_auth_token=True)\n",
        "\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "M4SXopIL26zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Preprocessing Dataset"
      ],
      "metadata": {
        "id": "9jMltjlo3MjC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# removing unnecessary columns\n",
        "common_voice = common_voice.remove_columns([\"accent\", \"age\", \"client_id\", \"down_votes\", \"gender\", \"locale\", \"path\", \"segment\", \"up_votes\"])\n",
        "print(common_voice)"
      ],
      "metadata": {
        "id": "UyPTV5YD3Lrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Pre-trained Feature Extractor\n",
        "\n",
        "In this cell, we import the WhisperFeatureExtractor from the transformers library and load a pre-trained feature extractor specifically designed for the Whisper model."
      ],
      "metadata": {
        "id": "h_rbW32f9ieX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-tiny\")"
      ],
      "metadata": {
        "id": "VBqsugul96-s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Pre-trained Tokenizer\n",
        "\n",
        "In this cell, we import the WhisperTokenizer from the transformers library and load a pre-trained tokenizer for the Whisper model."
      ],
      "metadata": {
        "id": "Avi7iMtGA39L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperTokenizer\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-tiny\", language=\"Urdu\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "miG1XTZo9-4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Pre-trained Processor\n",
        "\n",
        "In this cell, we import the WhisperProcessor from the transformers library and load a pre-trained Processor for the Whisper model."
      ],
      "metadata": {
        "id": "yiaiDjgrBCrl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperProcessor\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\", language=\"Urdu\", task=\"transcribe\")"
      ],
      "metadata": {
        "id": "DiegIsvo-R1M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remaining Data Processing"
      ],
      "metadata": {
        "id": "V9_Aq1HaBNYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "zOE7mWik-Pja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Audio\n",
        "# Set the sampling rate to 16000 Hz, which is a standard rate for speech processing.\n",
        "common_voice = common_voice.cast_column(\"audio\", Audio(sampling_rate=16000))"
      ],
      "metadata": {
        "id": "IBX9Ha8Z-V76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(common_voice[\"train\"][0])"
      ],
      "metadata": {
        "id": "XiwYoN-4-YYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(batch):\n",
        "    # load and resample audio data from 48 to 16kHz\n",
        "    audio = batch[\"audio\"]\n",
        "    # compute log-Mel input features from input audio array\n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "    # encode target text to label ids\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ],
      "metadata": {
        "id": "7eeQ_fzO-acH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the prepare_dataset function to each batch in the common_voice dataset.\n",
        "common_voice = common_voice.map(prepare_dataset, remove_columns=common_voice.column_names[\"train\"], num_proc=2)"
      ],
      "metadata": {
        "id": "vjNHk2Y4-c5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training and Evaluation"
      ],
      "metadata": {
        "id": "FXcZWcVJ-gMw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load Pre-trained Whisper Model\n",
        "\n",
        "In this cell, we import the `WhisperForConditionalGeneration` class from the `transformers` library and load a pre-trained Whisper model, specifically the \"whisper-tiny\" version from OpenAI.\n",
        "The `WhisperForConditionalGeneration` class is essential because it provides the architecture and methods needed for generating text from audio inputs."
      ],
      "metadata": {
        "id": "IRl0VcCqB8Tw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")"
      ],
      "metadata": {
        "id": "FqoH6jxW-fdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###configuring the model according to our requirements"
      ],
      "metadata": {
        "id": "KxhOHKeHCFeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.generation_config.language = \"urdu\"\n",
        "model.generation_config.task = \"transcribe\"\n",
        "\n",
        "model.generation_config.forced_decoder_ids = None"
      ],
      "metadata": {
        "id": "CF6BfQAd-qMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom Data Collator for Speech Sequence-to-Sequence Models\n",
        "\n",
        "In this cell, we define a custom data collator class DataCollatorSpeechSeq2SeqWithPadding using the @dataclass decorator. This collator is designed to handle the specific requirements of speech sequence-to-sequence models by **appropriately padding the input audio features and label sequences.** This custom collator ensures that the data is correctly formatted and padded for training the speech sequence-to-sequence model, facilitating efficient and accurate training.\n"
      ],
      "metadata": {
        "id": "1W-bN7sYCNL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "    decoder_start_token_id: int\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.decoder_start_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "\n",
        "        return batch"
      ],
      "metadata": {
        "id": "SGmkiy8m-_0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(\n",
        "    processor=processor,\n",
        "    decoder_start_token_id=model.config.decoder_start_token_id,\n",
        ")"
      ],
      "metadata": {
        "id": "FCodG_gr_CQv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation metrics\n",
        "import evaluate\n",
        "metric = evaluate.load(\"wer\")"
      ],
      "metadata": {
        "id": "bLGfPFsI_E4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# helping function to use the wer metrics\n",
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "    return {\"wer\": wer}"
      ],
      "metadata": {
        "id": "s7upgQmu_H9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The selection of hyperparameters was taken from hugging face platforms.\n",
        "\n",
        "Due to some contraints we changed some of them."
      ],
      "metadata": {
        "id": "lIdJhqFgDleS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-tiny-urdu\",  # This specifies the directory where the trained model and other outputs will be saved.\n",
        "    per_device_train_batch_size=16,  # The batch size for training per device (GPU or CPU). It determines the number of training samples processed simultaneously on each device during training.\n",
        "    gradient_accumulation_steps=2,  # Number of steps for which gradients are accumulated before performing a parameter update. Useful for effectively simulating larger batch sizes with limited memory.\n",
        "    learning_rate=1e-5,  # The initial learning rate for the optimizer.\n",
        "    warmup_steps=500,  # Number of steps for which the learning rate increases linearly from 0 to the specified learning rate. Helps stabilize training by gradually increasing the learning rate.\n",
        "    max_steps=1500,  # The maximum number of training steps to run. Training will stop when this number of steps is reached.\n",
        "    gradient_checkpointing=True,  # Whether to use gradient checkpointing to reduce memory usage during training. Trades off compute for memory.\n",
        "    fp16=True,  # Whether to use 16-bit precision (mixed precision training) to speed up training and reduce memory usage.\n",
        "    evaluation_strategy=\"steps\",  # Strategy for evaluation during training. \"steps\" means evaluation is performed every eval_steps steps.\n",
        "    per_device_eval_batch_size=8,  # The batch size for evaluation per device.\n",
        "    predict_with_generate=True,  # Whether to generate predictions using a generation strategy.\n",
        "    generation_max_length=225,  # Maximum length of the generated output sequences during prediction.\n",
        "    save_steps=500,  # Number of steps after which a checkpoint is saved.\n",
        "    eval_steps=500,  # Number of steps after which evaluation is performed during training.\n",
        "    logging_steps=25,  # Number of steps after which logs are written to the log file and printed on the console.\n",
        "    report_to=[\"tensorboard\"],  # Where to report training metrics. In this case, it's set to report to TensorBoard.\n",
        "    load_best_model_at_end=True,  # Whether to load the best model at the end of training based on the specified metric.\n",
        "    metric_for_best_model=\"wer\",  # The metric used to determine the best model during training.\n",
        "    greater_is_better=False,  # Whether a higher value of the specified metric indicates better performance.\n",
        "    push_to_hub=True,  # Whether to push the trained model to the Hugging Face Model Hub after training.\n",
        ")"
      ],
      "metadata": {
        "id": "NzqQIyes_KIi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=common_voice[\"train\"],\n",
        "    eval_dataset=common_voice[\"test\"],\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ],
      "metadata": {
        "id": "AOeNIR6S_MKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ],
      "metadata": {
        "id": "rHyvSRan_Oe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "some results here do not exactly match the one in research paper as the notebook was run again with different hyperparameters so the results are now changed but they are some close to original ones"
      ],
      "metadata": {
        "id": "q_71LqEjgUu5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "prvIAt24_UuT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## this is optional work if you want to push to hugging face as well."
      ],
      "metadata": {
        "id": "NMFWCJYW_egp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''kwargs = {\n",
        "    \"dataset_tags\": \"mozilla-foundation/common_voice_13_0\",\n",
        "    \"dataset\": \"Common Voice 11.0\",  # a 'pretty' name for the training dataset\n",
        "    \"dataset_args\": \"config: hi, split: test\",\n",
        "    \"language\": \"urdu\",\n",
        "    \"model_name\": \"Whisper Base Urdu - Taimoor Sardar\",  # a 'pretty' name for our model\n",
        "    \"finetuned_from\": \"openai/whisper-base\",\n",
        "    \"tasks\": \"automatic-speech-recognition\",\n",
        "}'''"
      ],
      "metadata": {
        "id": "U5kKWjN4_X8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#trainer.push_to_hub(**kwargs)"
      ],
      "metadata": {
        "id": "WHNVlL9x_aBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "pipe = pipeline(model=\"TS_TI/whisper-base-ur\")\n",
        "\n",
        "def transcribe(audio):\n",
        "    text = pipe(audio)[\"text\"]\n",
        "    return text\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=transcribe,\n",
        "    inputs=gr.Audio(source=\"microphone\", type=\"filepath\"),\n",
        "    outputs=\"text\",\n",
        "    title=\"Whisper base urdu\",\n",
        "    description=\"Realtime demo for Hindi speech recognition using a fine-tuned Whisper small model.\",\n",
        ")\n",
        "\n",
        "iface.launch()'''"
      ],
      "metadata": {
        "id": "RssQne_VIgB2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tv76pDhN-p8N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}